{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787883ad",
   "metadata": {},
   "source": [
    "We first load few library including the machine learning library Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1305aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa4ff3",
   "metadata": {},
   "source": [
    "We are loading a famous dataset in Machine Learning called **MNIST** \n",
    "\n",
    "MNIST are images of handwritten digits (from 0 to 9)\n",
    "\n",
    "Information about MNIST: https://en.wikipedia.org/wiki/MNIST_database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0864e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3f1a8",
   "metadata": {},
   "source": [
    "The size of `x_train` gives the number of instances in the dataset and the size of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7befcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ea33b",
   "metadata": {},
   "source": [
    "We pick up an arbitrary instance from the dataset to look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa588865",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 22876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[img])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e41fe0",
   "metadata": {},
   "source": [
    "Both arrays of data `x_train` and `y_train` have the same size: each **instance** (an image of a handwritten digit) has an associated **label**.\n",
    "\n",
    "For instance, for the arbitrary instance previously visualised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[img]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1e4f3",
   "metadata": {},
   "source": [
    "Finally, `x_test` is comprised of examples that are not included in `x_train`, and `y_test` are the labels associated to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a72802",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4a608",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bcf920",
   "metadata": {},
   "source": [
    "Let suppose that we have `x_train` comprised of datapoints e.g. images, and `y_train` in which each entry is associated with each entry of `x_train`, e.g. labels. We say that `x_train` are Ã¬nputs and `y_train` are `outputs` of the problem we want to model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd49f4",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5784c89",
   "metadata": {},
   "source": [
    "In supervissed ML, the goal is to find the mapping between *inputs* (data points) and their corresponding *outputs*. \n",
    "\n",
    "Taking the exemple of MNIST, a supervised ML algorithm applied to this dataset has to find a mapping between each image (`28 x 28` numbers) and its correponding label. Say differently, the algorithm has to find in the image what makes it belong to the class `0`, or `3` or `6` etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where(y_train == 6)[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b565df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(x_train[idx[0]])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(x_train[idx[1]])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(x_train[idx[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b7bc1a",
   "metadata": {},
   "source": [
    "### What for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd65ba",
   "metadata": {},
   "source": [
    "Once an aglorithm trained to understand the link between inputs and outputs, the same algorithm can then predict on an unseen datapoint (for instance an image) its likeliest output. \n",
    "\n",
    "If the outputs are labels (such as in MNIST), this task is called **classification**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a78a64-d1a1-41fa-ac05-2d63545843be",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/29728855/58266484-5cb2bb00-7d9f-11e9-864b-4574ab70ae4c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f54150",
   "metadata": {},
   "source": [
    "if the outputs are real numbers, this task is called **regression**. As an exemple of a regression task, consider that inputs are images of houses and outputs are prices. An algorithm trained on such dataset will build a mapping between the content of the image (size of the house, colours, etc..) and a price. If we give as input an unseen photo of a house, such algorithm should be able to predict its price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78af37-cc67-4714-a04a-91c7209a3564",
   "metadata": {},
   "source": [
    "![](https://pyimagesearch.com/wp-content/uploads/2019/01/keras_regression_css_header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67cead",
   "metadata": {},
   "source": [
    "### Assess performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519eec4",
   "metadata": {},
   "source": [
    "To assess a ML algorithm, we test its capacity to generalise to unseen data. Typically, we train a model with a `training` dataset (in case of MNIST, it is called `(x_train, y_train)`) and evaluate its performance on a `testing` dataset  (in case of MNIST, it is called `(x_test, y_test)`). Obviously, there are **no** common instance between a training and a testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85047f",
   "metadata": {},
   "source": [
    "To evaluate the performance, we use some metrics:\n",
    "* **Accuracy** is the fact to count the number of correct predictions divided by the total number of predictions made. Since we have the expected outputs `y_test` for the testing inputs `x_test`, it's easy to count the number of correct predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8773d3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{correct \\ predictions \\ on \\ x\\_test}{total \\ predictions}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86127a5a",
   "metadata": {},
   "source": [
    "* **Confusion matrix**: rows are the ground truth categories, columns are the predicted ones. A cell at (i,j) is the number of instances from class i that have been predicted as belonging to class j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d959f",
   "metadata": {},
   "source": [
    "![](https://qph.fs.quoracdn.net/main-qimg-7c4efb27dd2bd6d0c281d294b824ff4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511b3c6",
   "metadata": {},
   "source": [
    "* **TP, FP, FN, TN**: True positives, False positives, False negatives, True negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc41fbe",
   "metadata": {},
   "source": [
    "<div style=\"width:600px; text-align:center;\">\n",
    "<img src=\"https://www.researchgate.net/profile/D_Soeffker/publication/328860691/figure/fig2/AS:691499431907328@1541877720208/Confusion-matrix-and-related-performance-measures.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3ff08",
   "metadata": {},
   "source": [
    "## Exploring classification on MNIST using k-Neighrest Neighbour algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daffb44",
   "metadata": {},
   "source": [
    "Training a classifier on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e571bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = np.arange(60000)\n",
    "random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ed6ed",
   "metadata": {},
   "source": [
    "We will train a k-nearest neighbour algorithm. \n",
    "\n",
    "<div style=\"width:400px;\"><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/03/knn3.png\"></div>\n",
    "\n",
    "In a k-nearest neighbour algorithm (KNN), an input is classified regarding the proximity of its neighbors. The input will be predicted as belonging to the class C if the majority of the k-nearest neighbour belongs to class C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d31f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxtra = idx[:20000] # take only 20000 examples to reduce computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee147737",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train[idxtra], y_train[idxtra])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f5593",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26028027",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the trained algorithm by computing first its accuracy on the testing dataset given by `(x_test, y_test)`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a446b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe09f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7396d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        n_correct = n_correct + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = n_correct / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee51184",
   "metadata": {},
   "source": [
    "Note that there is a function to compute accuracy in sklearn: `score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10b693",
   "metadata": {},
   "source": [
    "We can also inspect the confusions between classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b894225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "predictions = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbabdd",
   "metadata": {},
   "source": [
    "And the F-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49feab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88cb9a1",
   "metadata": {},
   "source": [
    "### Inspect mis-classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821764f8",
   "metadata": {},
   "source": [
    "When assessing classification performance, we are not only interested in inspecting the number of good predictions. It is also insightful to inspect the examples that have been incorrectly classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd283d",
   "metadata": {},
   "source": [
    "Let's get the indexes of the misclassified examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = [k for k in range(len(y_pred)) if y_pred[k] != y_test[k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994191e",
   "metadata": {},
   "source": [
    "We plot the first five:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "for k in range(5):\n",
    "    plt.subplot(1, 5, k+1)\n",
    "    plt.imshow(x_test[misclassified[k]])\n",
    "    plt.title('Predicted as {}'.format(y_pred[misclassified[k]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284170c5",
   "metadata": {},
   "source": [
    "### Finding the best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6425bd",
   "metadata": {},
   "source": [
    "To find the best model, a common way in machine learning is to perform a *grid search* which means to train the model with various set of parameters and take the set of parameters that returns the best accuracy (or fscore). \n",
    "\n",
    "Such process is very time and ressource consuming! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06863b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for k in range(1, 8):\n",
    "    print('test {}'.format(k))\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train[idxtra], y_train[idxtra])\n",
    "    score = clf.score(X_test, y_test)\n",
    "    accuracies.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b485a",
   "metadata": {},
   "source": [
    "We then plot the accuracies along the values of parameter *k*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, len(accuracies)+1), accuracies, '-o')\n",
    "plt.ylabel('accuracies')\n",
    "plt.xlabel('k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7c74b",
   "metadata": {},
   "source": [
    "Conclusion: best model is for `k=?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928a0b2",
   "metadata": {},
   "source": [
    "## Exploring Deep Learning based classification on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbb7ed",
   "metadata": {},
   "source": [
    "State of the art algorithms are all based on Deep Neural Networks (DNNs). \n",
    "\n",
    "DNNs for classification looks like this:\n",
    "<div style=\"width:700px\"><img src=\"https://miro.medium.com/max/6694/1*Enbag4OPicgOFGP6m281lQ.jpeg\"></div>\n",
    "\n",
    "There are many different architectures of DNNs. The most famous one applied to image inputs are Convolutional Neural Networks (CNNs). That's the one will test on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383452c",
   "metadata": {},
   "source": [
    "The exemple shown comes from the keras tutorial that can be found here: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ff0d0",
   "metadata": {},
   "source": [
    "Before starting we have to prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train_dl = x_train[idxtra].astype(\"float32\") / 255\n",
    "x_test_dl = x_test.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79038d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train_dl = np.expand_dims(x_train_dl, -1)\n",
    "x_test_dl = np.expand_dims(x_test_dl, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train_dl = keras.utils.to_categorical(y_train[idxtra], 10)\n",
    "y_test_dl = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c432a",
   "metadata": {},
   "source": [
    "We use Keras, an interface for Tensorflow to easily create a Neural Network. Keras acts as an interface for Tensorflow, and allows to rapidly setup the architecture of a Neural Network by stacking layers of Neurons or activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow.keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(28,28,1)),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b970c8f",
   "metadata": {},
   "source": [
    "The neural network is comprised of trainable weights that must be optimized to learn the mapping between the inputs and the outputs. We provide training data in batches (groups) to the model and at each epoch (i.e. optimization step), the trainable weights of the network are updated according to the error made by the model. We call this process `gradient descent algorithm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = model.fit(x_train_dl, \n",
    "                 y_train_dl, \n",
    "                 batch_size=128, \n",
    "                 epochs=10, \n",
    "                 validation_split=0.1, \n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab051456",
   "metadata": {},
   "source": [
    "Now the Neural Network is trained, let's predict labels on unseed images and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl = model.predict(x_test_dl)\n",
    "n_correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == y_test[i]:\n",
    "        n_correct = n_correct + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_correct/len(y_pred)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(logs.history['loss'], '-o', label='training loss')\n",
    "plt.plot(logs.history['val_loss'], '-o', label='validation loss')\n",
    "plt.ylabel('LOSS')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(logs.history['accuracy'], '-o', label='training accuracy')\n",
    "plt.plot(logs.history['val_accuracy'], '-o', label='validation accuracy')\n",
    "plt.ylabel('ACCURACY')\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e09ce",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d98f3",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc300e",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0e2a6",
   "metadata": {},
   "source": [
    "In unsupervised machine learning, the goal is find structures in `x_train` without having access to `y_train`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c77ea",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76914c0e",
   "metadata": {},
   "source": [
    "Typical task in unsupervised machine learning is clustering, which is the task of grouping data in clusters of instances which are the closest (according to some metrics). However we do not have labels for this clusters. Typically, we arbitrarily give them an integer. \n",
    "\n",
    "Another typical task in unsupervised machine learning is *generation* where we provide examples that the algorithm tries to generate the best possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52ef75",
   "metadata": {},
   "source": [
    "## Clustering on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd34f9",
   "metadata": {},
   "source": [
    "We will illustrate clustering through the k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e05bd20",
   "metadata": {},
   "source": [
    "<div style=\"width:700px\"><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1200%2F1*rw8IUza1dbffBhiA4i0GNQ.png&f=1&nofb=1\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f962519",
   "metadata": {},
   "source": [
    "Declare and fit kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa84527",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust = KMeans(n_clusters=10, random_state=0, n_init=10)\n",
    "clust.fit(X_train[idxtra])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5854f7",
   "metadata": {},
   "source": [
    "We can then predict the cluster id for each example in the test set, the same way we did in classification when predicting the label associated to an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clust = clust.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff1c34",
   "metadata": {},
   "source": [
    "Quick and dirty test of the overlap between **classes** and **clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd981309",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_overlaps = 0\n",
    "for k in range(10):\n",
    "    idx = np.where(y_test == k)[0]\n",
    "    h = np.max([list(y_pred_clust[idx]).count(l) for l in range(10)])\n",
    "    n_overlaps += h\n",
    "print('Consistency: {:.2f}%'.format(n_overlaps/len(y_test)*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d79ed",
   "metadata": {},
   "source": [
    "### How to visualise the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(pca.transform(X_test)[:, 0], pca.transform(X_test)[:, 1], c=y_pred_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b836b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(pca.transform(X_test)[:, 0], pca.transform(X_test)[:, 1], c=y_pred_clust, alpha=0.05)\n",
    "plt.scatter(pca.transform(clust.cluster_centers_)[:, 0], pca.transform(clust.cluster_centers_)[:, 1], c=np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed19f49",
   "metadata": {},
   "source": [
    "### How good is the fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb823d22",
   "metadata": {},
   "source": [
    "There is no clear metrics to compute the goodness of fit because we don't have ground truth about the cluster in which belongs each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(pca.transform(X_test)[:, 0], pca.transform(X_test)[:, 1], c=y_pred_clust)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(pca.transform(X_test)[:, 0], pca.transform(X_test)[:, 1], c=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073b5ed-cf7c-4bc4-ae23-2350df217705",
   "metadata": {},
   "source": [
    "# Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044aaf95-d358-429f-a025-e9c43ac40187",
   "metadata": {},
   "source": [
    "In generative machine learning, the goal is be able to generate data that looks like what we have in `x_train` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd2540-8de9-4354-8532-d77cbf4b3934",
   "metadata": {},
   "source": [
    "A very famous approach (now a bit outdated) is the Generative Adversarial Networks (proposed in 2014, with model improvements until ~2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a43559-0fbd-41ae-a119-609abc1e5dc3",
   "metadata": {},
   "source": [
    "<div style=\"width:700px\"><img src=\"https://hyeongminlee.github.io/img/GAN_001/fig_1.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f887aaf-d87c-476b-9bd6-3e6fc58aa2ad",
   "metadata": {},
   "source": [
    "Example of a GAN trained on MNIST (taken from https://keras.io/examples/generative/conditional_gan/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7852104-ecc9-4308-ae28-f4ac03854bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import imageio\n",
    "from tensorflow_docs.vis import embed\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d52e6a-20d4-4612-81b0-f1d48bf4d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload MNIST Dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_labels = np.concatenate([y_train, y_test])\n",
    "\n",
    "# Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# the images, and one-hot encode the labels.\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "all_labels = keras.utils.to_categorical(all_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dc72b-4f3b-47a7-850f-c9adc045dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "print(\"Shape of training images:\", all_digits.shape)\n",
    "print(\"Shape of training labels\", all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a1970-d673-4e57-b753-51f332985b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085b78b-b2af-4e87-977a-54d062ca7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(7 * 7 * generator_in_channels),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e38b63-bf65-4ddf-bfac-32f9ef30abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = ops.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        )\n",
    "        image_one_hot_labels = ops.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = ops.shape(real_images)[0]\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
    "        )\n",
    "        random_vector_labels = ops.concatenate(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = ops.concatenate(\n",
    "            [generated_images, image_one_hot_labels], -1\n",
    "        )\n",
    "        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = ops.concatenate(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = ops.concatenate(\n",
    "            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = keras.random.normal(\n",
    "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
    "        )\n",
    "        random_vector_labels = ops.concatenate(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = ops.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = ops.concatenate(\n",
    "                [fake_images, image_one_hot_labels], -1\n",
    "            )\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20ae0f-f4b3-4b68-9cfb-2c394335557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gan = ConditionalGAN(discriminator=discriminator, \n",
    "                          generator=generator, \n",
    "                          latent_dim=latent_dim)\n",
    "\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e21e947-d6eb-45a1-8258-a0e428853bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model \n",
    "n_epochs = 1\n",
    "cond_gan.fit(dataset, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ab156-7644-45dc-a5f0-5dcc361f7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "json_config = cond_gan.generator.to_json()\n",
    "with open('generator_ep={:d}.config.json'.format(n_epochs), 'w') as f:\n",
    "    json.dump(json_config, f)\n",
    "cond_gan.generator.save_weights('generator_ep={:d}.weights.h5'.format(n_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473b707-1325-4a18-bc7e-54d23f9f7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights\n",
    "cond_gan.generator.load_weights('generator_ep=10.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c65bf-8b19-4cd6-96f7-e9478fb092d5",
   "metadata": {},
   "source": [
    "### Generate an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39906f23-2bde-4062-b202-fefbadebc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_noise = keras.random.normal(shape=(1, latent_dim))\n",
    "input_label = tf.convert_to_tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
    "noise_and_labels = ops.concatenate([input_noise, input_label], 1)\n",
    "fake = trained_gen.predict(noise_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d9853-df22-4b6f-bd26-49b16b22df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab19ac-d1c4-40b9-99c1-96e4270aa20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(6):\n",
    "    input_noise = keras.random.normal(shape=(1, latent_dim))\n",
    "    input_label = tf.convert_to_tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
    "    noise_and_labels = ops.concatenate([input_noise, input_label], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    plt.subplot(2, 3, int(k/3)*3+k%3+1)\n",
    "    plt.imshow(fake[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bff0a4-5122-4e7e-bfe9-7a0313f7a1da",
   "metadata": {},
   "source": [
    "### Interpolation between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f1a64-9cab-47b6-bb07-f606eeb5ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditional GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 9  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = keras.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = ops.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = ops.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = ops.cast(first_label, \"float32\")\n",
    "    second_label = ops.cast(second_label, \"float32\")\n",
    "    print(first_label)\n",
    "    print(second_label)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = ops.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = ops.cast(percent_second_label, \"float32\")\n",
    "    interpolation_labels = (first_label * (1 - percent_second_label) + second_label * percent_second_label)\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    print(interpolation_noise.shape, interpolation_labels.shape)\n",
    "    noise_and_labels = ops.concatenate([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n",
    "\n",
    "start_class = 2  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "end_class = 6  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "\n",
    "fake_images = interpolate_class(start_class, end_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b8628-db1c-400a-bc97-f3ae59d41427",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images *= 255.0\n",
    "converted_images = fake_images.astype(np.uint8)\n",
    "converted_images = ops.image.resize(converted_images, (96, 96)).numpy().astype(np.uint8)\n",
    "imageio.mimsave(\"animation.gif\", converted_images[:, :, :, 0], fps=1)\n",
    "embed.embed_file(\"animation.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8a036-af6e-4e6e-8b1b-47f09cc8ca09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
